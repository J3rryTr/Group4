{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:27.080075Z",
     "iopub.status.busy": "2024-04-01T23:04:27.079692Z",
     "iopub.status.idle": "2024-04-01T23:04:27.086512Z",
     "shell.execute_reply": "2024-04-01T23:04:27.085469Z",
     "shell.execute_reply.started": "2024-04-01T23:04:27.080045Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "from torch import autograd\n",
    "\n",
    "import time\n",
    "import _pickle as cPickle\n",
    "\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 80\n",
    "# plt.style.use('seaborn-pastel')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:27.088258Z",
     "iopub.status.busy": "2024-04-01T23:04:27.087937Z",
     "iopub.status.idle": "2024-04-01T23:04:27.101704Z",
     "shell.execute_reply": "2024-04-01T23:04:27.100925Z",
     "shell.execute_reply.started": "2024-04-01T23:04:27.088233Z"
    }
   },
   "outputs": [],
   "source": [
    "#parameters for the Model\n",
    "parameters = OrderedDict()\n",
    "parameters['train'] = \"/kaggle/input/hichic/train.txt\" #Path to train file\n",
    "parameters['dev'] = \"/kaggle/input/hichic/test.txt\" #Path to test file\n",
    "# parameters['test'] = \"./data/eng.testb\" #Path to dev file\n",
    "parameters['tag_scheme'] = \"BIO\" #BIO or BIOES\n",
    "parameters['lower'] = True # Boolean variable to control lowercasing of words\n",
    "parameters['zeros'] =  True # Boolean variable to control replacement of  all digits by 0 \n",
    "parameters['char_dim'] = 30 #Char embedding dimension\n",
    "parameters['word_dim'] = 100 #Token embedding dimension\n",
    "parameters['word_lstm_dim'] = 200 #Token LSTM hidden layer size\n",
    "parameters['word_bidirect'] = True #Use a bidirectional LSTM for words\n",
    "parameters['embedding_path'] = \"/kaggle/input/glove6b100dtxt/glove.6B.100d.txt\" #Location of pretrained embeddings\n",
    "parameters['all_emb'] = 1 #Load all embeddings\n",
    "parameters['crf'] =1 #Use CRF (0 to disable)\n",
    "parameters['dropout'] = 0.5 #Droupout on the input (0 = no dropout)\n",
    "parameters['epoch'] =  50 #Number of epochs to run\"\n",
    "parameters['weights'] = \"\" #path to Pretrained for from a previous run\n",
    "parameters['name'] = \"self-trained-model\" # Model name\n",
    "parameters['gradient_clip']=5.0\n",
    "parameters['char_mode']=\"CNN\"\n",
    "models_path = \"./models/\" #path to saved models\n",
    "\n",
    "#GPU\n",
    "parameters['use_gpu'] = torch.cuda.is_available() #GPU Check\n",
    "use_gpu = parameters['use_gpu']\n",
    "\n",
    "parameters['reload'] = None #\"./models/pre-trained-model\" \n",
    "\n",
    "#Constants\n",
    "START_TAG = '<START>'\n",
    "STOP_TAG = '<STOP>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:27.103374Z",
     "iopub.status.busy": "2024-04-01T23:04:27.103102Z",
     "iopub.status.idle": "2024-04-01T23:04:27.110527Z",
     "shell.execute_reply": "2024-04-01T23:04:27.109757Z",
     "shell.execute_reply.started": "2024-04-01T23:04:27.103351Z"
    }
   },
   "outputs": [],
   "source": [
    "#paths to files \n",
    "#To stored mapping file\n",
    "mapping_file = 'mapping.pkl'\n",
    "\n",
    "#To stored model\n",
    "name = parameters['name']\n",
    "model_name = models_path + name #get_name(parameters)\n",
    "\n",
    "if not os.path.exists(models_path):\n",
    "    os.makedirs(models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:27.111796Z",
     "iopub.status.busy": "2024-04-01T23:04:27.111534Z",
     "iopub.status.idle": "2024-04-01T23:04:27.125089Z",
     "shell.execute_reply": "2024-04-01T23:04:27.124280Z",
     "shell.execute_reply.started": "2024-04-01T23:04:27.111767Z"
    }
   },
   "outputs": [],
   "source": [
    "def zero_digits(s):\n",
    "    \"\"\"\n",
    "    Replace every digit in a string by a zero.\n",
    "    \"\"\"\n",
    "    return re.sub('\\d', '0', s)\n",
    "\n",
    "def load_sentences(path, zeros):\n",
    "    \"\"\"\n",
    "    Load sentences. A line must contain at least a word and its tag.\n",
    "    Sentences are separated by empty lines.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in codecs.open(path, 'r', 'utf8'):\n",
    "        line = zero_digits(line.rstrip()) if zeros else line.rstrip()\n",
    "        if not line:\n",
    "            if len(sentence) > 0:\n",
    "                if 'DOCSTART' not in sentence[0][0]:\n",
    "                    sentences.append(sentence)\n",
    "                sentence = []\n",
    "        else:\n",
    "            word = line.split()\n",
    "            assert len(word) >= 2\n",
    "            sentence.append(word)\n",
    "    if len(sentence) > 0:\n",
    "        if 'DOCSTART' not in sentence[0][0]:\n",
    "            sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:27.127560Z",
     "iopub.status.busy": "2024-04-01T23:04:27.127137Z",
     "iopub.status.idle": "2024-04-01T23:04:28.574622Z",
     "shell.execute_reply": "2024-04-01T23:04:28.573821Z",
     "shell.execute_reply.started": "2024-04-01T23:04:27.127528Z"
    }
   },
   "outputs": [],
   "source": [
    "train_sentences = load_sentences(parameters['train'], parameters['zeros'])\n",
    "# test_sentences = load_sentences(parameters['test'], parameters['zeros'])\n",
    "dev_sentences = load_sentences(parameters['dev'], parameters['zeros'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:28.576002Z",
     "iopub.status.busy": "2024-04-01T23:04:28.575708Z",
     "iopub.status.idle": "2024-04-01T23:04:28.589580Z",
     "shell.execute_reply": "2024-04-01T23:04:28.588707Z",
     "shell.execute_reply.started": "2024-04-01T23:04:28.575969Z"
    }
   },
   "outputs": [],
   "source": [
    "def iob2(tags):\n",
    "    \"\"\"\n",
    "    Check that tags have a valid BIO format.\n",
    "    Tags in BIO1 format are converted to BIO2.\n",
    "    \"\"\"\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag == 'O':\n",
    "            continue\n",
    "        split = tag.split('-')\n",
    "        if len(split) != 2 or split[0] not in ['I', 'B']:\n",
    "            return False\n",
    "        if split[0] == 'B':\n",
    "            continue\n",
    "        elif i == 0 or tags[i - 1] == 'O':  # conversion IOB1 to IOB2\n",
    "            tags[i] = 'B' + tag[1:]\n",
    "        elif tags[i - 1][1:] == tag[1:]:\n",
    "            continue\n",
    "        else:  # conversion IOB1 to IOB2\n",
    "            tags[i] = 'B' + tag[1:]\n",
    "    return True\n",
    "\n",
    "def iob_iobes(tags):\n",
    "    \"\"\"\n",
    "    the function is used to convert\n",
    "    BIO -> BIOES tagging\n",
    "    \"\"\"\n",
    "    new_tags = []\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag == 'O':\n",
    "            new_tags.append(tag)\n",
    "        elif tag.split('-')[0] == 'B':\n",
    "            if i + 1 != len(tags) and \\\n",
    "               tags[i + 1].split('-')[0] == 'I':\n",
    "                new_tags.append(tag)\n",
    "            else:\n",
    "                new_tags.append(tag.replace('B-', 'S-'))\n",
    "        elif tag.split('-')[0] == 'I':\n",
    "            if i + 1 < len(tags) and \\\n",
    "                    tags[i + 1].split('-')[0] == 'I':\n",
    "                new_tags.append(tag)\n",
    "            else:\n",
    "                new_tags.append(tag.replace('I-', 'E-'))\n",
    "        else:\n",
    "            raise Exception('Invalid IOB format!')\n",
    "    return new_tags\n",
    "\n",
    "def update_tag_scheme(sentences, tag_scheme):\n",
    "    \"\"\"\n",
    "    Check and update sentences tagging scheme to BIO2\n",
    "    Only BIO1 and BIO2 schemes are accepted for input data.\n",
    "    \"\"\"\n",
    "    for i, s in enumerate(sentences):\n",
    "        tags = [w[-1] for w in s]\n",
    "        # Check that tags are given in the BIO format\n",
    "        if not iob2(tags):\n",
    "            s_str = '\\n'.join(' '.join(w) for w in s)\n",
    "            raise Exception('Sentences should be given in BIO format! ' +\n",
    "                            'Please check sentence %i:\\n%s' % (i, s_str))\n",
    "        if tag_scheme == 'BIOES':\n",
    "            new_tags = iob_iobes(tags)\n",
    "            for word, new_tag in zip(s, new_tags):\n",
    "                word[-1] = new_tag\n",
    "        else:\n",
    "            raise Exception('Wrong tagging scheme!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:28.591716Z",
     "iopub.status.busy": "2024-04-01T23:04:28.591451Z",
     "iopub.status.idle": "2024-04-01T23:04:28.601851Z",
     "shell.execute_reply": "2024-04-01T23:04:28.601134Z",
     "shell.execute_reply.started": "2024-04-01T23:04:28.591693Z"
    }
   },
   "outputs": [],
   "source": [
    "# update_tag_scheme(train_sentences, parameters['tag_scheme'])\n",
    "# update_tag_scheme(dev_sentences, parameters['tag_scheme'])\n",
    "# # update_tag_scheme(test_sentences, parameters['tag_scheme'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:28.603518Z",
     "iopub.status.busy": "2024-04-01T23:04:28.603223Z",
     "iopub.status.idle": "2024-04-01T23:04:28.616102Z",
     "shell.execute_reply": "2024-04-01T23:04:28.615158Z",
     "shell.execute_reply.started": "2024-04-01T23:04:28.603486Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dico(item_list):\n",
    "    \"\"\"\n",
    "    Create a dictionary of items from a list of list of items.\n",
    "    \"\"\"\n",
    "    assert type(item_list) is list\n",
    "    dico = {}\n",
    "    for items in item_list:\n",
    "        for item in items:\n",
    "            if item not in dico:\n",
    "                dico[item] = 1\n",
    "            else:\n",
    "                dico[item] += 1\n",
    "    return dico\n",
    "\n",
    "def create_mapping(dico):\n",
    "    \"\"\"\n",
    "    Create a mapping (item to ID / ID to item) from a dictionary.\n",
    "    Items are ordered by decreasing frequency.\n",
    "    \"\"\"\n",
    "    sorted_items = sorted(dico.items(), key=lambda x: (-x[1], x[0]))\n",
    "    id_to_item = {i: v[0] for i, v in enumerate(sorted_items)}\n",
    "    item_to_id = {v: k for k, v in id_to_item.items()}\n",
    "    return item_to_id, id_to_item\n",
    "\n",
    "def word_mapping(sentences, lower):\n",
    "    \"\"\"\n",
    "    Create a dictionary and a mapping of words, sorted by frequency.\n",
    "    \"\"\"\n",
    "    words = [[x[0].lower() if lower else x[0] for x in s] for s in sentences]\n",
    "    dico = create_dico(words)\n",
    "    dico['<UNK>'] = 10000000 #UNK tag for unknown words\n",
    "    word_to_id, id_to_word = create_mapping(dico)\n",
    "    print(\"Found %i unique words (%i in total)\" % (\n",
    "        len(dico), sum(len(x) for x in words)\n",
    "    ))\n",
    "    return dico, word_to_id, id_to_word\n",
    "\n",
    "def char_mapping(sentences):\n",
    "    \"\"\"\n",
    "    Create a dictionary and mapping of characters, sorted by frequency.\n",
    "    \"\"\"\n",
    "    chars = [\"\".join([w[0] for w in s]) for s in sentences]\n",
    "    dico = create_dico(chars)\n",
    "    char_to_id, id_to_char = create_mapping(dico)\n",
    "    print(\"Found %i unique characters\" % len(dico))\n",
    "    return dico, char_to_id, id_to_char\n",
    "\n",
    "def tag_mapping(sentences):\n",
    "    \"\"\"\n",
    "    Create a dictionary and a mapping of tags, sorted by frequency.\n",
    "    \"\"\"\n",
    "    tags = [[word[-1] for word in s] for s in sentences]\n",
    "    dico = create_dico(tags)\n",
    "    dico[START_TAG] = -1\n",
    "    dico[STOP_TAG] = -2\n",
    "    tag_to_id, id_to_tag = create_mapping(dico)\n",
    "    print(\"Found %i unique named entity tags\" % len(dico))\n",
    "    return dico, tag_to_id, id_to_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:28.617953Z",
     "iopub.status.busy": "2024-04-01T23:04:28.617357Z",
     "iopub.status.idle": "2024-04-01T23:04:28.962968Z",
     "shell.execute_reply": "2024-04-01T23:04:28.962056Z",
     "shell.execute_reply.started": "2024-04-01T23:04:28.617917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8730 unique words (188887 in total)\n",
      "Found 109 unique characters\n",
      "Found 84 unique named entity tags\n"
     ]
    }
   ],
   "source": [
    "dico_words,word_to_id,id_to_word = word_mapping(train_sentences, parameters['lower'])\n",
    "dico_chars, char_to_id, id_to_char = char_mapping(train_sentences)\n",
    "dico_tags, tag_to_id, id_to_tag = tag_mapping(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:28.964694Z",
     "iopub.status.busy": "2024-04-01T23:04:28.964322Z",
     "iopub.status.idle": "2024-04-01T23:04:28.970081Z",
     "shell.execute_reply": "2024-04-01T23:04:28.969117Z",
     "shell.execute_reply.started": "2024-04-01T23:04:28.964660Z"
    }
   },
   "outputs": [],
   "source": [
    "def lower_case(x,lower=False):\n",
    "    if lower:\n",
    "        return x.lower()  \n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:28.971602Z",
     "iopub.status.busy": "2024-04-01T23:04:28.971283Z",
     "iopub.status.idle": "2024-04-01T23:04:29.820494Z",
     "shell.execute_reply": "2024-04-01T23:04:29.819434Z",
     "shell.execute_reply.started": "2024-04-01T23:04:28.971571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7896 / 788 sentences in train / dev.\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(sentences, word_to_id, char_to_id, tag_to_id, lower=False):\n",
    "    \"\"\"\n",
    "    Prepare the dataset. Return a list of lists of dictionaries containing:\n",
    "        - word indexes\n",
    "        - word char indexes\n",
    "        - tag indexes\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for s in sentences:\n",
    "        str_words = [w[0] for w in s]\n",
    "        words = [word_to_id[lower_case(w,lower) if lower_case(w,lower) in word_to_id else '<UNK>']\n",
    "                 for w in str_words]\n",
    "        # Skip characters that are not in the training set\n",
    "        chars = [[char_to_id[c] for c in w if c in char_to_id]\n",
    "                 for w in str_words]\n",
    "        tags = [tag_to_id[w[-1]] for w in s]\n",
    "        data.append({\n",
    "            'str_words': str_words,\n",
    "            'words': words,\n",
    "            'chars': chars,\n",
    "            'tags': tags,\n",
    "        })\n",
    "    return data\n",
    "\n",
    "train_data = prepare_dataset(\n",
    "    train_sentences, word_to_id, char_to_id, tag_to_id, parameters['lower']\n",
    ")\n",
    "dev_data = prepare_dataset(\n",
    "    dev_sentences, word_to_id, char_to_id, tag_to_id, parameters['lower']\n",
    ")\n",
    "# test_data = prepare_dataset(\n",
    "#     test_sentences, word_to_id, char_to_id, tag_to_id, parameters['lower']\n",
    "# )\n",
    "print(\"{} / {} sentences in train / dev.\".format(len(train_data), len(dev_data))) #len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:29.823878Z",
     "iopub.status.busy": "2024-04-01T23:04:29.823573Z",
     "iopub.status.idle": "2024-04-01T23:04:51.161202Z",
     "shell.execute_reply": "2024-04-01T23:04:51.160241Z",
     "shell.execute_reply.started": "2024-04-01T23:04:29.823851Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 pretrained embeddings.\n"
     ]
    }
   ],
   "source": [
    "all_word_embeds = {}\n",
    "for i, line in enumerate(codecs.open(parameters['embedding_path'], 'r', 'utf-8')):\n",
    "    s = line.strip().split()\n",
    "    if len(s) == parameters['word_dim'] + 1:\n",
    "        all_word_embeds[s[0]] = np.array([float(i) for i in s[1:]])\n",
    "\n",
    "#Intializing Word Embedding Matrix\n",
    "word_embeds = np.random.uniform(-np.sqrt(0.06), np.sqrt(0.06), (len(word_to_id), parameters['word_dim']))\n",
    "\n",
    "for w in word_to_id:\n",
    "    if w in all_word_embeds:\n",
    "        word_embeds[word_to_id[w]] = all_word_embeds[w]\n",
    "    elif w.lower() in all_word_embeds:\n",
    "        word_embeds[word_to_id[w]] = all_word_embeds[w.lower()]\n",
    "\n",
    "print('Loaded %i pretrained embeddings.' % len(all_word_embeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:51.163377Z",
     "iopub.status.busy": "2024-04-01T23:04:51.162508Z",
     "iopub.status.idle": "2024-04-01T23:04:51.184102Z",
     "shell.execute_reply": "2024-04-01T23:04:51.183187Z",
     "shell.execute_reply.started": "2024-04-01T23:04:51.163338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_to_id:  8730\n"
     ]
    }
   ],
   "source": [
    "with open(mapping_file, 'wb') as f:\n",
    "    mappings = {\n",
    "        'word_to_id': word_to_id,\n",
    "        'tag_to_id': tag_to_id,\n",
    "        'char_to_id': char_to_id,\n",
    "        'parameters': parameters,\n",
    "        'word_embeds': word_embeds\n",
    "    }\n",
    "    cPickle.dump(mappings, f)\n",
    "\n",
    "print('word_to_id: ', len(word_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:51.185561Z",
     "iopub.status.busy": "2024-04-01T23:04:51.185304Z",
     "iopub.status.idle": "2024-04-01T23:04:51.190010Z",
     "shell.execute_reply": "2024-04-01T23:04:51.189179Z",
     "shell.execute_reply.started": "2024-04-01T23:04:51.185537Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_embedding(input_embedding):\n",
    "    \"\"\"\n",
    "    Initialize embedding\n",
    "    \"\"\"\n",
    "    bias = np.sqrt(3.0 / input_embedding.size(1))\n",
    "    nn.init.uniform(input_embedding, -bias, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:51.191875Z",
     "iopub.status.busy": "2024-04-01T23:04:51.191299Z",
     "iopub.status.idle": "2024-04-01T23:04:51.199195Z",
     "shell.execute_reply": "2024-04-01T23:04:51.198297Z",
     "shell.execute_reply.started": "2024-04-01T23:04:51.191841Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_linear(input_linear):\n",
    "    \"\"\"\n",
    "    Initialize linear transformation\n",
    "    \"\"\"\n",
    "    bias = np.sqrt(6.0 / (input_linear.weight.size(0) + input_linear.weight.size(1)))\n",
    "    nn.init.uniform(input_linear.weight, -bias, bias)\n",
    "    if input_linear.bias is not None:\n",
    "        input_linear.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:51.200656Z",
     "iopub.status.busy": "2024-04-01T23:04:51.200400Z",
     "iopub.status.idle": "2024-04-01T23:04:51.215300Z",
     "shell.execute_reply": "2024-04-01T23:04:51.214490Z",
     "shell.execute_reply.started": "2024-04-01T23:04:51.200634Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_lstm(input_lstm):\n",
    "    \"\"\"\n",
    "    Initialize lstm\n",
    "    \n",
    "    PyTorch weights parameters:\n",
    "    \n",
    "        weight_ih_l[k]: the learnable input-hidden weights of the k-th layer,\n",
    "            of shape `(hidden_size * input_size)` for `k = 0`. Otherwise, the shape is\n",
    "            `(hidden_size * hidden_size)`\n",
    "            \n",
    "        weight_hh_l[k]: the learnable hidden-hidden weights of the k-th layer,\n",
    "            of shape `(hidden_size * hidden_size)`            \n",
    "    \"\"\"\n",
    "    \n",
    "    # Weights init for forward layer\n",
    "    for ind in range(0, input_lstm.num_layers):\n",
    "        \n",
    "        ## Gets the weights Tensor from our model, for the input-hidden weights in our current layer\n",
    "        weight = eval('input_lstm.weight_ih_l' + str(ind))\n",
    "        \n",
    "        # Initialize the sampling range\n",
    "        sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "        \n",
    "        # Randomly sample from our samping range using uniform distribution and apply it to our current layer\n",
    "        nn.init.uniform(weight, -sampling_range, sampling_range)\n",
    "        \n",
    "        # Similar to above but for the hidden-hidden weights of the current layer\n",
    "        weight = eval('input_lstm.weight_hh_l' + str(ind))\n",
    "        sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "        nn.init.uniform(weight, -sampling_range, sampling_range)\n",
    "        \n",
    "        \n",
    "    # We do the above again, for the backward layer if we are using a bi-directional LSTM (our final model uses this)\n",
    "    if input_lstm.bidirectional:\n",
    "        for ind in range(0, input_lstm.num_layers):\n",
    "            weight = eval('input_lstm.weight_ih_l' + str(ind) + '_reverse')\n",
    "            sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "            nn.init.uniform(weight, -sampling_range, sampling_range)\n",
    "            weight = eval('input_lstm.weight_hh_l' + str(ind) + '_reverse')\n",
    "            sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "            nn.init.uniform(weight, -sampling_range, sampling_range)\n",
    "\n",
    "    # Bias initialization steps\n",
    "    \n",
    "    # We initialize them to zero except for the forget gate bias, which is initialized to 1\n",
    "    if input_lstm.bias:\n",
    "        for ind in range(0, input_lstm.num_layers):\n",
    "            bias = eval('input_lstm.bias_ih_l' + str(ind))\n",
    "            \n",
    "            # Initializing to zero\n",
    "            bias.data.zero_()\n",
    "            \n",
    "            # This is the range of indices for our forget gates for each LSTM cell\n",
    "            bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
    "            \n",
    "            #Similar for the hidden-hidden layer\n",
    "            bias = eval('input_lstm.bias_hh_l' + str(ind))\n",
    "            bias.data.zero_()\n",
    "            bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
    "            \n",
    "        # Similar to above, we do for backward layer if we are using a bi-directional LSTM \n",
    "        if input_lstm.bidirectional:\n",
    "            for ind in range(0, input_lstm.num_layers):\n",
    "                bias = eval('input_lstm.bias_ih_l' + str(ind) + '_reverse')\n",
    "                bias.data.zero_()\n",
    "                bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
    "                bias = eval('input_lstm.bias_hh_l' + str(ind) + '_reverse')\n",
    "                bias.data.zero_()\n",
    "                bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:51.216827Z",
     "iopub.status.busy": "2024-04-01T23:04:51.216443Z",
     "iopub.status.idle": "2024-04-01T23:04:51.229101Z",
     "shell.execute_reply": "2024-04-01T23:04:51.228167Z",
     "shell.execute_reply.started": "2024-04-01T23:04:51.216796Z"
    }
   },
   "outputs": [],
   "source": [
    "def log_sum_exp(vec):\n",
    "    '''\n",
    "    This function calculates the score explained above for the forward algorithm\n",
    "    vec 2D: 1 * tagset_size\n",
    "    '''\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "    \n",
    "def argmax(vec):\n",
    "    '''\n",
    "    This function returns the max index in a vector\n",
    "    '''\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return to_scalar(idx)\n",
    "\n",
    "def to_scalar(var):\n",
    "    '''\n",
    "    Function to convert pytorch tensor to a scalar\n",
    "    '''\n",
    "    return var.view(-1).data.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:51.230427Z",
     "iopub.status.busy": "2024-04-01T23:04:51.230177Z",
     "iopub.status.idle": "2024-04-01T23:04:51.239814Z",
     "shell.execute_reply": "2024-04-01T23:04:51.239060Z",
     "shell.execute_reply.started": "2024-04-01T23:04:51.230405Z"
    }
   },
   "outputs": [],
   "source": [
    "def score_sentences(self, feats, tags):\n",
    "    # tags is ground_truth, a list of ints, length is len(sentence)\n",
    "    # feats is a 2D tensor, len(sentence) * tagset_size\n",
    "    r = torch.LongTensor(range(feats.size()[0]))\n",
    "    if self.use_gpu:\n",
    "        r = r.cuda()\n",
    "        pad_start_tags = torch.cat([torch.cuda.LongTensor([self.tag_to_ix[START_TAG]]), tags])\n",
    "        pad_stop_tags = torch.cat([tags, torch.cuda.LongTensor([self.tag_to_ix[STOP_TAG]])])\n",
    "    else:\n",
    "        pad_start_tags = torch.cat([torch.LongTensor([self.tag_to_ix[START_TAG]]), tags])\n",
    "        pad_stop_tags = torch.cat([tags, torch.LongTensor([self.tag_to_ix[STOP_TAG]])])\n",
    "\n",
    "    score = torch.sum(self.transitions[pad_stop_tags, pad_start_tags]) + torch.sum(feats[r, tags])\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:51.241046Z",
     "iopub.status.busy": "2024-04-01T23:04:51.240791Z",
     "iopub.status.idle": "2024-04-01T23:04:51.253823Z",
     "shell.execute_reply": "2024-04-01T23:04:51.252936Z",
     "shell.execute_reply.started": "2024-04-01T23:04:51.241023Z"
    }
   },
   "outputs": [],
   "source": [
    "def forward_alg(self, feats):\n",
    "    '''\n",
    "    This function performs the forward algorithm explained above\n",
    "    '''\n",
    "    # calculate in log domain\n",
    "    # feats is len(sentence) * tagset_size\n",
    "    # initialize alpha with a Tensor with values all equal to -10000.\n",
    "    \n",
    "    # Do the forward algorithm to compute the partition function\n",
    "    init_alphas = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n",
    "    \n",
    "    # START_TAG has all of the score.\n",
    "    init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "    \n",
    "    # Wrap in a variable so that we will get automatic backprop\n",
    "    forward_var = autograd.Variable(init_alphas)\n",
    "    if self.use_gpu:\n",
    "        forward_var = forward_var.cuda()\n",
    "        \n",
    "    # Iterate through the sentence\n",
    "    for feat in feats:\n",
    "        # broadcast the emission score: it is the same regardless of\n",
    "        # the previous tag\n",
    "        emit_score = feat.view(-1, 1)\n",
    "        \n",
    "        # the ith entry of trans_score is the score of transitioning to\n",
    "        # next_tag from i\n",
    "        tag_var = forward_var + self.transitions + emit_score\n",
    "        \n",
    "        # The ith entry of next_tag_var is the value for the\n",
    "        # edge (i -> next_tag) before we do log-sum-exp\n",
    "        max_tag_var, _ = torch.max(tag_var, dim=1)\n",
    "        \n",
    "        # The forward variable for this tag is log-sum-exp of all the\n",
    "        # scores.\n",
    "        tag_var = tag_var - max_tag_var.view(-1, 1)\n",
    "        \n",
    "        # Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "        forward_var = max_tag_var + torch.log(torch.sum(torch.exp(tag_var), dim=1)).view(1, -1) # ).view(1, -1)\n",
    "    terminal_var = (forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]).view(1, -1)\n",
    "    alpha = log_sum_exp(terminal_var)\n",
    "    # Z(x)\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:51.255473Z",
     "iopub.status.busy": "2024-04-01T23:04:51.255065Z",
     "iopub.status.idle": "2024-04-01T23:04:51.268800Z",
     "shell.execute_reply": "2024-04-01T23:04:51.268035Z",
     "shell.execute_reply.started": "2024-04-01T23:04:51.255442Z"
    }
   },
   "outputs": [],
   "source": [
    "def viterbi_algo(self, feats):\n",
    "    '''\n",
    "    In this function, we implement the viterbi algorithm explained above.\n",
    "    A Dynamic programming based approach to find the best tag sequence\n",
    "    '''\n",
    "    backpointers = []\n",
    "    # analogous to forward\n",
    "    \n",
    "    # Initialize the viterbi variables in log space\n",
    "    init_vvars = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n",
    "    init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "    \n",
    "    # forward_var at step i holds the viterbi variables for step i-1\n",
    "    forward_var = Variable(init_vvars)\n",
    "    if self.use_gpu:\n",
    "        forward_var = forward_var.cuda()\n",
    "    for feat in feats:\n",
    "        next_tag_var = forward_var.view(1, -1).expand(self.tagset_size, self.tagset_size) + self.transitions\n",
    "        _, bptrs_t = torch.max(next_tag_var, dim=1)\n",
    "        bptrs_t = bptrs_t.squeeze().data.cpu().numpy() # holds the backpointers for this step\n",
    "        next_tag_var = next_tag_var.data.cpu().numpy() \n",
    "        viterbivars_t = next_tag_var[range(len(bptrs_t)), bptrs_t] # holds the viterbi variables for this step\n",
    "        viterbivars_t = Variable(torch.FloatTensor(viterbivars_t))\n",
    "        if self.use_gpu:\n",
    "            viterbivars_t = viterbivars_t.cuda()\n",
    "            \n",
    "        # Now add in the emission scores, and assign forward_var to the set\n",
    "        # of viterbi variables we just computed\n",
    "        forward_var = viterbivars_t + feat\n",
    "        backpointers.append(bptrs_t)\n",
    "\n",
    "    # Transition to STOP_TAG\n",
    "    terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "    terminal_var.data[self.tag_to_ix[STOP_TAG]] = -10000.\n",
    "    terminal_var.data[self.tag_to_ix[START_TAG]] = -10000.\n",
    "    best_tag_id = argmax(terminal_var.unsqueeze(0))\n",
    "    path_score = terminal_var[best_tag_id]\n",
    "    \n",
    "    # Follow the back pointers to decode the best path.\n",
    "    best_path = [best_tag_id]\n",
    "    for bptrs_t in reversed(backpointers):\n",
    "        best_tag_id = bptrs_t[best_tag_id]\n",
    "        best_path.append(best_tag_id)\n",
    "        \n",
    "    # Pop off the start tag (we dont want to return that to the caller)\n",
    "    start = best_path.pop()\n",
    "    assert start == self.tag_to_ix[START_TAG] # Sanity check\n",
    "    best_path.reverse()\n",
    "    return path_score, best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:51.270333Z",
     "iopub.status.busy": "2024-04-01T23:04:51.269822Z",
     "iopub.status.idle": "2024-04-01T23:04:51.281536Z",
     "shell.execute_reply": "2024-04-01T23:04:51.280618Z",
     "shell.execute_reply.started": "2024-04-01T23:04:51.270302Z"
    }
   },
   "outputs": [],
   "source": [
    "def forward_calc(self, sentence, chars, chars2_length, d):\n",
    "    \n",
    "    '''\n",
    "    The function calls viterbi decode and generates the \n",
    "    most probable sequence of tags for the sentence\n",
    "    '''\n",
    "    \n",
    "    # Get the emission scores from the BiLSTM\n",
    "    feats = self._get_lstm_features(sentence, chars, chars2_length, d)\n",
    "    # viterbi to get tag_seq\n",
    "    \n",
    "    # Find the best path, given the features.\n",
    "    if self.use_crf:\n",
    "        score, tag_seq = self.viterbi_decode(feats)\n",
    "    else:\n",
    "        score, tag_seq = torch.max(feats, 1)\n",
    "        tag_seq = list(tag_seq.cpu().data)\n",
    "\n",
    "    return score, tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:51.283060Z",
     "iopub.status.busy": "2024-04-01T23:04:51.282781Z",
     "iopub.status.idle": "2024-04-01T23:04:51.297521Z",
     "shell.execute_reply": "2024-04-01T23:04:51.296747Z",
     "shell.execute_reply.started": "2024-04-01T23:04:51.283037Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_lstm_features(self, sentence, chars2, chars2_length, d):\n",
    "    \n",
    "    if self.char_mode == 'LSTM':\n",
    "        \n",
    "            chars_embeds = self.char_embeds(chars2).transpose(0, 1)\n",
    "            \n",
    "            packed = torch.nn.utils.rnn.pack_padded_sequence(chars_embeds, chars2_length)\n",
    "            \n",
    "            lstm_out, _ = self.char_lstm(packed)\n",
    "            \n",
    "            outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(lstm_out)\n",
    "            \n",
    "            outputs = outputs.transpose(0, 1)\n",
    "            \n",
    "            chars_embeds_temp = Variable(torch.FloatTensor(torch.zeros((outputs.size(0), outputs.size(2)))))\n",
    "            \n",
    "            if self.use_gpu:\n",
    "                chars_embeds_temp = chars_embeds_temp.cuda()\n",
    "            \n",
    "            for i, index in enumerate(output_lengths):\n",
    "                chars_embeds_temp[i] = torch.cat((outputs[i, index-1, :self.char_lstm_dim], outputs[i, 0, self.char_lstm_dim:]))\n",
    "            \n",
    "            chars_embeds = chars_embeds_temp.clone()\n",
    "            \n",
    "            for i in range(chars_embeds.size(0)):\n",
    "                chars_embeds[d[i]] = chars_embeds_temp[i]\n",
    "    \n",
    "    \n",
    "    if self.char_mode == 'CNN':\n",
    "        chars_embeds = self.char_embeds(chars2).unsqueeze(1)\n",
    "\n",
    "        ## Creating Character level representation using Convolutional Neural Netowrk\n",
    "        ## followed by a Maxpooling Layer\n",
    "        chars_cnn_out3 = self.char_cnn3(chars_embeds)\n",
    "        chars_embeds = nn.functional.max_pool2d(chars_cnn_out3,\n",
    "                                             kernel_size=(chars_cnn_out3.size(2), 1)).view(chars_cnn_out3.size(0), self.out_channels)\n",
    "\n",
    "        ## Loading word embeddings\n",
    "    embeds = self.word_embeds(sentence)\n",
    "\n",
    "    ## We concatenate the word embeddings and the character level representation\n",
    "    ## to create unified representation for each word\n",
    "    embeds = torch.cat((embeds, chars_embeds), 1)\n",
    "\n",
    "    embeds = embeds.unsqueeze(1)\n",
    "\n",
    "    ## Dropout on the unified embeddings\n",
    "    embeds = self.dropout(embeds)\n",
    "\n",
    "    ## Word lstm\n",
    "    ## Takes words as input and generates a output at each step\n",
    "    lstm_out, _ = self.lstm(embeds)\n",
    "\n",
    "    ## Reshaping the outputs from the lstm layer\n",
    "    lstm_out = lstm_out.view(len(sentence), self.hidden_dim*2)\n",
    "\n",
    "    ## Dropout on the lstm output\n",
    "    lstm_out = self.dropout(lstm_out)\n",
    "\n",
    "    ## Linear layer converts the ouput vectors to tag space\n",
    "    lstm_feats = self.hidden2tag(lstm_out)\n",
    "    \n",
    "    return lstm_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:51.298735Z",
     "iopub.status.busy": "2024-04-01T23:04:51.298488Z",
     "iopub.status.idle": "2024-04-01T23:04:51.310361Z",
     "shell.execute_reply": "2024-04-01T23:04:51.309564Z",
     "shell.execute_reply.started": "2024-04-01T23:04:51.298713Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_neg_log_likelihood(self, sentence, tags, chars2, chars2_length, d):\n",
    "    # sentence, tags is a list of ints\n",
    "    # features is a 2D tensor, len(sentence) * self.tagset_size\n",
    "    feats = self._get_lstm_features(sentence, chars2, chars2_length, d)\n",
    "\n",
    "    if self.use_crf:\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "    else:\n",
    "        tags = Variable(tags)\n",
    "        scores = nn.functional.cross_entropy(feats, tags)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:51.311775Z",
     "iopub.status.busy": "2024-04-01T23:04:51.311519Z",
     "iopub.status.idle": "2024-04-01T23:04:51.329797Z",
     "shell.execute_reply": "2024-04-01T23:04:51.328932Z",
     "shell.execute_reply.started": "2024-04-01T23:04:51.311753Z"
    }
   },
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim,\n",
    "                 char_to_ix=None, pre_word_embeds=None, char_out_dimension=25,char_embedding_dim=25, use_gpu=False\n",
    "                 , use_crf=True, char_mode='CNN'):\n",
    "        '''\n",
    "        Input parameters:\n",
    "                \n",
    "                vocab_size= Size of vocabulary (int)\n",
    "                tag_to_ix = Dictionary that maps NER tags to indices\n",
    "                embedding_dim = Dimension of word embeddings (int)\n",
    "                hidden_dim = The hidden dimension of the LSTM layer (int)\n",
    "                char_to_ix = Dictionary that maps characters to indices\n",
    "                pre_word_embeds = Numpy array which provides mapping from word embeddings to word indices\n",
    "                char_out_dimension = Output dimension from the CNN encoder for character\n",
    "                char_embedding_dim = Dimension of the character embeddings\n",
    "                use_gpu = defines availability of GPU, \n",
    "                    when True: CUDA function calls are made\n",
    "                    else: Normal CPU function calls are made\n",
    "                use_crf = parameter which decides if you want to use the CRF layer for output decoding\n",
    "        '''\n",
    "        \n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        \n",
    "        #parameter initialization for the model\n",
    "        self.use_gpu = use_gpu\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.use_crf = use_crf\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "        self.out_channels = char_out_dimension\n",
    "        self.char_mode = char_mode\n",
    "\n",
    "        if char_embedding_dim is not None:\n",
    "            self.char_embedding_dim = char_embedding_dim\n",
    "            \n",
    "            #Initializing the character embedding layer\n",
    "            self.char_embeds = nn.Embedding(len(char_to_ix), char_embedding_dim)\n",
    "            init_embedding(self.char_embeds.weight)\n",
    "            \n",
    "            #Performing LSTM encoding on the character embeddings\n",
    "            if self.char_mode == 'LSTM':\n",
    "                self.char_lstm = nn.LSTM(char_embedding_dim, char_lstm_dim, num_layers=1, bidirectional=True)\n",
    "                init_lstm(self.char_lstm)\n",
    "                \n",
    "            #Performing CNN encoding on the character embeddings\n",
    "            if self.char_mode == 'CNN':\n",
    "                self.char_cnn3 = nn.Conv2d(in_channels=1, out_channels=self.out_channels, kernel_size=(3, char_embedding_dim), padding=(2,0))\n",
    "\n",
    "        #Creating Embedding layer with dimension of ( number of words * dimension of each word)\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if pre_word_embeds is not None:\n",
    "            #Initializes the word embeddings with pretrained word embeddings\n",
    "            self.pre_word_embeds = True\n",
    "            self.word_embeds.weight = nn.Parameter(torch.FloatTensor(pre_word_embeds))\n",
    "        else:\n",
    "            self.pre_word_embeds = False\n",
    "    \n",
    "        #Initializing the dropout layer, with dropout specificed in parameters\n",
    "        self.dropout = nn.Dropout(parameters['dropout'])\n",
    "        \n",
    "        #Lstm Layer:\n",
    "        #input dimension: word embedding dimension + character level representation\n",
    "        #bidirectional=True, specifies that we are using the bidirectional LSTM\n",
    "        if self.char_mode == 'LSTM':\n",
    "            self.lstm = nn.LSTM(embedding_dim+char_lstm_dim*2, hidden_dim, bidirectional=True)\n",
    "        if self.char_mode == 'CNN':\n",
    "            self.lstm = nn.LSTM(embedding_dim+self.out_channels, hidden_dim, bidirectional=True)\n",
    "        \n",
    "        #Initializing the lstm layer using predefined function for initialization\n",
    "        init_lstm(self.lstm)\n",
    "        \n",
    "        # Linear layer which maps the output of the bidirectional LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, self.tagset_size)\n",
    "        \n",
    "        #Initializing the linear layer using predefined function for initialization\n",
    "        init_linear(self.hidden2tag) \n",
    "\n",
    "        if self.use_crf:\n",
    "            # Matrix of transition parameters.  Entry i,j is the score of transitioning *to* i *from* j.\n",
    "            # Matrix has a dimension of (total number of tags * total number of tags)\n",
    "            self.transitions = nn.Parameter(\n",
    "                torch.zeros(self.tagset_size, self.tagset_size))\n",
    "            \n",
    "            # These two statements enforce the constraint that we never transfer\n",
    "            # to the start tag and we never transfer from the stop tag\n",
    "            self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "            self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "    #assigning the functions, which we have defined earlier\n",
    "    _score_sentence = score_sentences\n",
    "    _get_lstm_features = get_lstm_features\n",
    "    _forward_alg = forward_alg\n",
    "    viterbi_decode = viterbi_algo\n",
    "    neg_log_likelihood = get_neg_log_likelihood\n",
    "    forward = forward_calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:51.331334Z",
     "iopub.status.busy": "2024-04-01T23:04:51.330903Z",
     "iopub.status.idle": "2024-04-01T23:04:51.362397Z",
     "shell.execute_reply": "2024-04-01T23:04:51.361514Z",
     "shell.execute_reply.started": "2024-04-01T23:04:51.331302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34/3399668671.py:6: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  nn.init.uniform(input_embedding, -bias, bias)\n",
      "/tmp/ipykernel_34/2828957834.py:25: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  nn.init.uniform(weight, -sampling_range, sampling_range)\n",
      "/tmp/ipykernel_34/2828957834.py:30: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  nn.init.uniform(weight, -sampling_range, sampling_range)\n",
      "/tmp/ipykernel_34/2828957834.py:38: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  nn.init.uniform(weight, -sampling_range, sampling_range)\n",
      "/tmp/ipykernel_34/2828957834.py:41: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  nn.init.uniform(weight, -sampling_range, sampling_range)\n"
     ]
    }
   ],
   "source": [
    "#creating the model using the Class defined above\n",
    "model = BiLSTM_CRF(vocab_size=len(word_to_id),\n",
    "                   tag_to_ix=tag_to_id,\n",
    "                   embedding_dim=parameters['word_dim'],\n",
    "                   hidden_dim=parameters['word_lstm_dim'],\n",
    "                   use_gpu=use_gpu,\n",
    "                   char_to_ix=char_to_id,\n",
    "                   pre_word_embeds=word_embeds,\n",
    "                   use_crf=parameters['crf'],\n",
    "                   char_mode=parameters['char_mode'])\n",
    "print(\"Model Initialized!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:51.363981Z",
     "iopub.status.busy": "2024-04-01T23:04:51.363705Z",
     "iopub.status.idle": "2024-04-01T23:04:51.370936Z",
     "shell.execute_reply": "2024-04-01T23:04:51.369953Z",
     "shell.execute_reply.started": "2024-04-01T23:04:51.363959Z"
    }
   },
   "outputs": [],
   "source": [
    "# #Reload a saved model, if parameter[\"reload\"] is set to a path\n",
    "# if parameters['reload']:\n",
    "#     if not os.path.exists(parameters['reload']):\n",
    "#         print(\"downloading pre-trained model\")\n",
    "#         model_url=\"https://github.com/TheAnig/NER-LSTM-CNN-Pytorch/raw/master/trained-model-cpu\"\n",
    "#         urllib.request.urlretrieve(model_url, parameters['reload'])\n",
    "#     model.load_state_dict(torch.load(parameters['reload']))\n",
    "#     print(\"model reloaded :\", parameters['reload'])\n",
    "\n",
    "if use_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:51.375911Z",
     "iopub.status.busy": "2024-04-01T23:04:51.375616Z",
     "iopub.status.idle": "2024-04-01T23:04:51.384819Z",
     "shell.execute_reply": "2024-04-01T23:04:51.383955Z",
     "shell.execute_reply.started": "2024-04-01T23:04:51.375887Z"
    }
   },
   "outputs": [],
   "source": [
    "#Initializing the optimizer\n",
    "#The best results in the paper where achived using stochastic gradient descent (SGD) \n",
    "#learning rate=0.015 and momentum=0.9 \n",
    "#decay_rate=0.05 \n",
    "\n",
    "learning_rate = 0.015\n",
    "momentum = 0.9\n",
    "number_of_epochs = parameters['epoch'] \n",
    "decay_rate = 0.05\n",
    "gradient_clip = parameters['gradient_clip']\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "#variables which will used in training process\n",
    "losses = [] #list to store all losses\n",
    "loss = 0.0 #Loss Initializatoin\n",
    "best_dev_F = -1.0 # Current best F-1 Score on Dev Set\n",
    "best_test_F = -1.0 # Current best F-1 Score on Test Set\n",
    "best_train_F = -1.0 # Current best F-1 Score on Train Set\n",
    "all_F = [[0, 0, 0]] # List storing all the F-1 Scores\n",
    "eval_every = len(train_data) # Calculate F-1 Score after this many iterations\n",
    "plot_every = 2000 # Store loss after this many iterations\n",
    "count = 0 #Counts the number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:51.386368Z",
     "iopub.status.busy": "2024-04-01T23:04:51.385956Z",
     "iopub.status.idle": "2024-04-01T23:04:51.398105Z",
     "shell.execute_reply": "2024-04-01T23:04:51.397359Z",
     "shell.execute_reply.started": "2024-04-01T23:04:51.386336Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_chunk_type(tok, idx_to_tag):\n",
    "    \"\"\"\n",
    "    The function takes in a chunk (\"B-PER\") and then splits it into the tag (PER) and its class (B)\n",
    "    as defined in BIOES\n",
    "    \n",
    "    Args:\n",
    "        tok: id of token, ex 4\n",
    "        idx_to_tag: dictionary {4: \"B-PER\", ...}\n",
    "\n",
    "    Returns:\n",
    "        tuple: \"B\", \"PER\"\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    tag_name = idx_to_tag[tok]\n",
    "    tag_class = tag_name.split('-')[0]\n",
    "    tag_type = tag_name.split('-')[-1]\n",
    "    return tag_class, tag_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:51.399949Z",
     "iopub.status.busy": "2024-04-01T23:04:51.399357Z",
     "iopub.status.idle": "2024-04-01T23:04:51.410215Z",
     "shell.execute_reply": "2024-04-01T23:04:51.409280Z",
     "shell.execute_reply.started": "2024-04-01T23:04:51.399917Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_chunks(seq, tags):\n",
    "    \"\"\"Given a sequence of tags, group entities and their position\n",
    "\n",
    "    Args:\n",
    "        seq: [4, 4, 0, 0, ...] sequence of labels\n",
    "        tags: dict[\"O\"] = 4\n",
    "\n",
    "    Returns:\n",
    "        list of (chunk_type, chunk_start, chunk_end)\n",
    "\n",
    "    Example:\n",
    "        seq = [4, 5, 0, 3]\n",
    "        tags = {\"B-PER\": 4, \"I-PER\": 5, \"B-LOC\": 3}\n",
    "        result = [(\"PER\", 0, 2), (\"LOC\", 3, 4)]\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # We assume by default the tags lie outside a named entity\n",
    "    default = tags[\"O\"]\n",
    "    \n",
    "    idx_to_tag = {idx: tag for tag, idx in tags.items()}\n",
    "    \n",
    "    chunks = []\n",
    "    \n",
    "    chunk_type, chunk_start = None, None\n",
    "    for i, tok in enumerate(seq):\n",
    "        # End of a chunk 1\n",
    "        if tok == default and chunk_type is not None:\n",
    "            # Add a chunk.\n",
    "            chunk = (chunk_type, chunk_start, i)\n",
    "            chunks.append(chunk)\n",
    "            chunk_type, chunk_start = None, None\n",
    "\n",
    "        # End of a chunk + start of a chunk!\n",
    "        elif tok != default:\n",
    "            tok_chunk_class, tok_chunk_type = get_chunk_type(tok, idx_to_tag)\n",
    "            if chunk_type is None:\n",
    "                # Initialize chunk for each entity\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "            elif tok_chunk_type != chunk_type or tok_chunk_class == \"B\":\n",
    "                # If chunk class is B, i.e., its a beginning of a new named entity\n",
    "                # or, if the chunk type is different from the previous one, then we\n",
    "                # start labelling it as a new entity\n",
    "                chunk = (chunk_type, chunk_start, i)\n",
    "                chunks.append(chunk)\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # end condition\n",
    "    if chunk_type is not None:\n",
    "        chunk = (chunk_type, chunk_start, len(seq))\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:51.411951Z",
     "iopub.status.busy": "2024-04-01T23:04:51.411662Z",
     "iopub.status.idle": "2024-04-01T23:04:51.429612Z",
     "shell.execute_reply": "2024-04-01T23:04:51.428821Z",
     "shell.execute_reply.started": "2024-04-01T23:04:51.411928Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluating(model, datas, best_F,dataset=\"Train\"):\n",
    "    '''\n",
    "    The function takes as input the model, data and calcuates F-1 Score\n",
    "    It performs conditional updates \n",
    "     1) Flag to save the model \n",
    "     2) Best F-1 score\n",
    "    ,if the F-1 score calculated improves on the previous F-1 score\n",
    "    '''\n",
    "    # Initializations\n",
    "    prediction = [] # A list that stores predicted tags\n",
    "    save = False # Flag that tells us if the model needs to be saved\n",
    "    new_F = 0.0 # Variable to store the current F1-Score (may not be the best)\n",
    "    correct_preds, total_correct, total_preds = 0., 0., 0. # Count variables\n",
    "    \n",
    "    for data in datas:\n",
    "        ground_truth_id = data['tags']\n",
    "        words = data['str_words']\n",
    "        chars2 = data['chars']\n",
    "        \n",
    "        if parameters['char_mode'] == 'LSTM':\n",
    "            chars2_sorted = sorted(chars2, key=lambda p: len(p), reverse=True)\n",
    "            d = {}\n",
    "            for i, ci in enumerate(chars2):\n",
    "                for j, cj in enumerate(chars2_sorted):\n",
    "                    if ci == cj and not j in d and not i in d.values():\n",
    "                        d[j] = i\n",
    "                        continue\n",
    "            chars2_length = [len(c) for c in chars2_sorted]\n",
    "            char_maxl = max(chars2_length)\n",
    "            chars2_mask = np.zeros((len(chars2_sorted), char_maxl), dtype='int')\n",
    "            for i, c in enumerate(chars2_sorted):\n",
    "                chars2_mask[i, :chars2_length[i]] = c\n",
    "            chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "        \n",
    "        \n",
    "        if parameters['char_mode'] == 'CNN':\n",
    "            d = {} \n",
    "\n",
    "            # Padding the each word to max word size of that sentence\n",
    "            chars2_length = [len(c) for c in chars2]\n",
    "            char_maxl = max(chars2_length)\n",
    "            chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n",
    "            for i, c in enumerate(chars2):\n",
    "                chars2_mask[i, :chars2_length[i]] = c\n",
    "            chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "\n",
    "        dwords = Variable(torch.LongTensor(data['words']))\n",
    "        \n",
    "        # We are getting the predicted output from our model\n",
    "        if use_gpu:\n",
    "            val,out = model(dwords.cuda(), chars2_mask.cuda(), chars2_length, d)\n",
    "        else:\n",
    "            val,out = model(dwords, chars2_mask, chars2_length, d)\n",
    "        predicted_id = out\n",
    "    \n",
    "        \n",
    "        # We use the get chunks function defined above to get the true chunks\n",
    "        # and the predicted chunks from true labels and predicted labels respectively\n",
    "        lab_chunks      = set(get_chunks(ground_truth_id,tag_to_id))\n",
    "        lab_pred_chunks = set(get_chunks(predicted_id,\n",
    "                                         tag_to_id))\n",
    "\n",
    "        # Updating the count variables\n",
    "        correct_preds += len(lab_chunks & lab_pred_chunks)\n",
    "        total_preds   += len(lab_pred_chunks)\n",
    "        total_correct += len(lab_chunks)\n",
    "    \n",
    "    # Calculating the F1-Score\n",
    "    p   = correct_preds / total_preds if correct_preds > 0 else 0\n",
    "    r   = correct_preds / total_correct if correct_preds > 0 else 0\n",
    "    new_F  = 2 * p * r / (p + r) if correct_preds > 0 else 0\n",
    "\n",
    "    print(\"{}: new_F: {} best_F: {} \".format(dataset,new_F,best_F))\n",
    "    \n",
    "    # If our current F1-Score is better than the previous best, we update the best\n",
    "    # to current F1 and we set the flag to indicate that we need to checkpoint this model\n",
    "    \n",
    "    if new_F>best_F:\n",
    "        best_F=new_F\n",
    "        save=True\n",
    "\n",
    "    return best_F, new_F, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:51.430953Z",
     "iopub.status.busy": "2024-04-01T23:04:51.430614Z",
     "iopub.status.idle": "2024-04-01T23:04:51.443860Z",
     "shell.execute_reply": "2024-04-01T23:04:51.442982Z",
     "shell.execute_reply.started": "2024-04-01T23:04:51.430930Z"
    }
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, lr):\n",
    "    \"\"\"\n",
    "    shrink learning rate\n",
    "    \"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:51.445283Z",
     "iopub.status.busy": "2024-04-01T23:04:51.444902Z",
     "iopub.status.idle": "2024-04-01T23:04:51.453353Z",
     "shell.execute_reply": "2024-04-01T23:04:51.452535Z",
     "shell.execute_reply.started": "2024-04-01T23:04:51.445248Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T23:04:51.457315Z",
     "iopub.status.busy": "2024-04-01T23:04:51.456973Z",
     "iopub.status.idle": "2024-04-02T00:59:41.952112Z",
     "shell.execute_reply": "2024-04-02T00:59:41.951168Z",
     "shell.execute_reply.started": "2024-04-01T23:04:51.457292Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/49 [00:00<?, ?it/s]/tmp/ipykernel_34/488635563.py:58: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), gradient_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 :  1.1681817141576034\n",
      "4000 :  0.7494110174555972\n",
      "6000 :  0.6653895256303735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 1/49 [01:39<1:19:56, 99.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 :  0.6251491115870862\n",
      "10000 :  0.5737573464682251\n",
      "12000 :  0.5637619677203891\n",
      "14000 :  0.5474264204754026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 2/49 [03:19<1:18:15, 99.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000 :  0.5137404903488776\n",
      "18000 :  0.5035526609727542\n",
      "20000 :  0.4898979325441753\n",
      "22000 :  0.47187962948809203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 3/49 [04:59<1:16:26, 99.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 :  0.46371408247043155\n",
      "26000 :  0.43655458546347337\n",
      "28000 :  0.46554833501547244\n",
      "30000 :  0.4466031875725379\n",
      "Train: new_F: 0.5976873342175067 best_F: -1.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 4/49 [07:37<1:32:06, 122.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.5902633074445918 best_F: -1.0 \n",
      "Saving Model to  ./models/self-trained-model\n",
      "32000 :  0.43877326737178696\n",
      "34000 :  0.40251186751953233\n",
      "36000 :  0.4097165423632261\n",
      "38000 :  0.41919069155803246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 5/49 [09:17<1:23:56, 114.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 :  0.4057332950721482\n",
      "42000 :  0.39222334166597445\n",
      "44000 :  0.3744868041126642\n",
      "46000 :  0.37871093840364894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 6/49 [10:57<1:18:36, 109.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000 :  0.36035133993947094\n",
      "50000 :  0.355291322340627\n",
      "52000 :  0.36795612102472813\n",
      "54000 :  0.3495388573821492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|        | 7/49 [12:38<1:14:41, 106.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56000 :  0.32936907851424474\n",
      "58000 :  0.33379603716791534\n",
      "60000 :  0.3308224789573818\n",
      "62000 :  0.33490491772624653\n",
      "Train: new_F: 0.7000869805036384 best_F: 0.5976873342175067 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|        | 8/49 [15:16<1:24:12, 123.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.6453968864116856 best_F: 0.5902633074445918 \n",
      "Saving Model to  ./models/self-trained-model\n",
      "64000 :  0.3144835500474576\n",
      "66000 :  0.3029915199241598\n",
      "68000 :  0.3073039290644781\n",
      "70000 :  0.30799816112371203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|        | 9/49 [16:57<1:17:30, 116.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72000 :  0.2941066712702122\n",
      "74000 :  0.2916821980026652\n",
      "76000 :  0.29233436055823436\n",
      "78000 :  0.29333523689679003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|        | 10/49 [18:37<1:12:12, 111.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000 :  0.27882359658092565\n",
      "82000 :  0.27944155702558304\n",
      "84000 :  0.2675969177175822\n",
      "86000 :  0.27768934442472354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 11/49 [20:16<1:08:09, 107.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88000 :  0.2576853665573045\n",
      "90000 :  0.2703383111057424\n",
      "92000 :  0.2548478986810451\n",
      "94000 :  0.2577744844152494\n",
      "Train: new_F: 0.7672926177965373 best_F: 0.7000869805036384 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|       | 12/49 [22:56<1:16:07, 123.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.6954990215264187 best_F: 0.6453968864116856 \n",
      "Saving Model to  ./models/self-trained-model\n",
      "96000 :  0.25790650618235306\n",
      "98000 :  0.24893616807953167\n",
      "100000 :  0.26604027091414145\n",
      "102000 :  0.2506823933471082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|       | 13/49 [24:37<1:09:54, 116.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104000 :  0.22587528990789563\n",
      "106000 :  0.2418976626156994\n",
      "108000 :  0.2428857149925385\n",
      "110000 :  0.24123682742194802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 14/49 [26:18<1:05:16, 111.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112000 :  0.2219594196117345\n",
      "114000 :  0.23087715180026983\n",
      "116000 :  0.22632402448292332\n",
      "118000 :  0.24011771496421913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|       | 15/49 [27:59<1:01:36, 108.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000 :  0.21234829355736598\n",
      "122000 :  0.21511082696899264\n",
      "124000 :  0.2363591207467305\n",
      "126000 :  0.21193334099148567\n",
      "Train: new_F: 0.8096170897168897 best_F: 0.7672926177965373 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 16/49 [30:39<1:08:13, 124.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.7307763572355012 best_F: 0.6954990215264187 \n",
      "Saving Model to  ./models/self-trained-model\n",
      "128000 :  0.20339713519569863\n",
      "130000 :  0.20954765583986734\n",
      "132000 :  0.21240944851375768\n",
      "134000 :  0.2185485513435538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|      | 17/49 [32:20<1:02:29, 117.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136000 :  0.20099977848192285\n",
      "138000 :  0.1987815229444787\n",
      "140000 :  0.2107695781667484\n",
      "142000 :  0.2050238323321307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|      | 18/49 [34:01<57:57, 112.19s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144000 :  0.2033483367829201\n",
      "146000 :  0.18653778489709116\n",
      "148000 :  0.19363052604124253\n",
      "150000 :  0.20624876551720042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|      | 19/49 [35:40<54:12, 108.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152000 :  0.18437945957350743\n",
      "154000 :  0.19215830143708668\n",
      "156000 :  0.18949813052044737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|      | 20/49 [37:23<51:35, 106.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158000 :  0.18298574379660376\n",
      "160000 :  0.18247019941633363\n",
      "162000 :  0.18171305856068687\n",
      "164000 :  0.1897505545920278\n",
      "Train: new_F: 0.8566069977285514 best_F: 0.8096170897168897 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|     | 21/49 [40:03<57:14, 122.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.765491651205937 best_F: 0.7307763572355012 \n",
      "Saving Model to  ./models/self-trained-model\n",
      "166000 :  0.1817060820609237\n",
      "168000 :  0.17611091079077915\n",
      "170000 :  0.18074039851306117\n",
      "172000 :  0.18178874362374875\n",
      "Train: new_F: 0.8584000996305368 best_F: 0.8566069977285514 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|     | 22/49 [42:43<1:00:14, 133.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.7674074074074074 best_F: 0.765491651205937 \n",
      "Saving Model to  ./models/self-trained-model\n",
      "174000 :  0.17853612283124295\n",
      "176000 :  0.16845711251394843\n",
      "178000 :  0.16744439816326004\n",
      "180000 :  0.1733767751187936\n",
      "Train: new_F: 0.865238622840494 best_F: 0.8584000996305368 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|     | 23/49 [45:23<1:01:24, 141.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.7727603787327021 best_F: 0.7674074074074074 \n",
      "Saving Model to  ./models/self-trained-model\n",
      "182000 :  0.1832130973500852\n",
      "184000 :  0.17177117535075787\n",
      "186000 :  0.1723976172516925\n",
      "188000 :  0.16135749807918534\n",
      "Train: new_F: 0.8743001061549028 best_F: 0.865238622840494 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|     | 24/49 [48:03<1:01:20, 147.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.7736165455561768 best_F: 0.7727603787327021 \n",
      "Saving Model to  ./models/self-trained-model\n",
      "190000 :  0.16161572029616036\n",
      "192000 :  0.16062119708604233\n",
      "194000 :  0.16504487822027736\n",
      "196000 :  0.16103248362685615\n",
      "Train: new_F: 0.87711102383364 best_F: 0.8743001061549028 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|     | 25/49 [50:42<1:00:20, 150.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.7824963072378139 best_F: 0.7736165455561768 \n",
      "Saving Model to  ./models/self-trained-model\n",
      "198000 :  0.1754028370916047\n",
      "200000 :  0.15966925309056557\n",
      "202000 :  0.16249400227410427\n",
      "204000 :  0.15717277629853724\n",
      "Train: new_F: 0.884067290339978 best_F: 0.87711102383364 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|    | 26/49 [53:21<58:43, 153.22s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.7910170749814401 best_F: 0.7824963072378139 \n",
      "Saving Model to  ./models/self-trained-model\n",
      "206000 :  0.16092815746421335\n",
      "208000 :  0.16149342967239103\n",
      "210000 :  0.15264237088056137\n",
      "212000 :  0.15603970599169567\n",
      "Train: new_F: 0.8878855036142581 best_F: 0.884067290339978 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|    | 27/49 [56:00<56:46, 154.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.7927795174065206 best_F: 0.7910170749814401 \n",
      "Saving Model to  ./models/self-trained-model\n",
      "214000 :  0.1543323856094031\n",
      "216000 :  0.14448068851856308\n",
      "218000 :  0.1469383683252344\n",
      "220000 :  0.1564888292138621\n",
      "Train: new_F: 0.8909482045341718 best_F: 0.8878855036142581 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|    | 28/49 [58:40<54:44, 156.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.8016893132574368 best_F: 0.7927795174065206 \n",
      "Saving Model to  ./models/self-trained-model\n",
      "222000 :  0.1509001837216658\n",
      "224000 :  0.14939777054488926\n",
      "226000 :  0.14443091220732307\n",
      "228000 :  0.14008932518290976\n",
      "Train: new_F: 0.896623118491066 best_F: 0.8909482045341718 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|    | 29/49 [1:01:19<52:27, 157.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.8026485194040831 best_F: 0.8016893132574368 \n",
      "Saving Model to  ./models/self-trained-model\n",
      "230000 :  0.1459300626047507\n",
      "232000 :  0.14888978693814\n",
      "234000 :  0.14867512337110775\n",
      "236000 :  0.14455866932810207\n",
      "Train: new_F: 0.8976153607928151 best_F: 0.896623118491066 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|    | 30/49 [1:03:59<50:05, 158.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.8030606667881217 best_F: 0.8026485194040831 \n",
      "Saving Model to  ./models/self-trained-model\n",
      "238000 :  0.1424283981355824\n",
      "240000 :  0.14553536149568405\n",
      "242000 :  0.14019907621484665\n",
      "244000 :  0.13783736140338537\n",
      "Train: new_F: 0.8975068108643606 best_F: 0.8976153607928151 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|   | 31/49 [1:06:39<47:35, 158.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.8071454611738971 best_F: 0.8030606667881217 \n",
      "Saving Model to  ./models/self-trained-model\n",
      "246000 :  0.13757542204907278\n",
      "248000 :  0.13926792219192713\n",
      "250000 :  0.14042646432138506\n",
      "252000 :  0.13591611649068405\n",
      "Train: new_F: 0.9062822257971372 best_F: 0.8976153607928151 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|   | 32/49 [1:09:18<44:58, 158.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.8140171564154042 best_F: 0.8071454611738971 \n",
      "Saving Model to  ./models/self-trained-model\n",
      "254000 :  0.1359040695287154\n",
      "256000 :  0.12474532841799542\n",
      "258000 :  0.139375995237929\n",
      "260000 :  0.1345369939368029\n",
      "Train: new_F: 0.9128803668195082 best_F: 0.9062822257971372 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|   | 33/49 [1:11:57<42:19, 158.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.8224819143016138 best_F: 0.8140171564154042 \n",
      "Saving Model to  ./models/self-trained-model\n",
      "262000 :  0.13563804562726187\n",
      "264000 :  0.13060983067672843\n",
      "266000 :  0.12830648405278172\n",
      "268000 :  0.1269582042185768\n",
      "Train: new_F: 0.9162465509014336 best_F: 0.9128803668195082 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|   | 34/49 [1:14:37<39:48, 159.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.8243292907019478 best_F: 0.8224819143016138 \n",
      "Saving Model to  ./models/self-trained-model\n",
      "270000 :  0.13108725075218897\n",
      "272000 :  0.1308567106872814\n",
      "274000 :  0.12576542417437866\n",
      "276000 :  0.12823190731736492\n",
      "Train: new_F: 0.9144528193088677 best_F: 0.9162465509014336 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|  | 35/49 [1:17:19<37:18, 159.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.8223110465116279 best_F: 0.8243292907019478 \n",
      "278000 :  0.1301907594231954\n",
      "280000 :  0.12728982773252587\n",
      "282000 :  0.12723115691585027\n",
      "284000 :  0.12344097778235638\n",
      "Train: new_F: 0.9185470669899091 best_F: 0.9162465509014336 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|  | 36/49 [1:20:01<34:48, 160.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.8251186564439577 best_F: 0.8243292907019478 \n",
      "Saving Model to  ./models/self-trained-model\n",
      "286000 :  0.1229073784468765\n",
      "288000 :  0.1204293456954669\n",
      "290000 :  0.12381488430719374\n",
      "292000 :  0.11965876135333253\n",
      "Train: new_F: 0.9238532300600121 best_F: 0.9185470669899091 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|  | 37/49 [1:22:44<32:15, 161.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.8295559240832872 best_F: 0.8251186564439577 \n",
      "Saving Model to  ./models/self-trained-model\n",
      "294000 :  0.11796069264752296\n",
      "296000 :  0.12411747694458022\n",
      "298000 :  0.12110253025725135\n",
      "300000 :  0.11996541919160289\n",
      "Train: new_F: 0.9262720664589823 best_F: 0.9238532300600121 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 38/49 [1:25:25<29:32, 161.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.8288454444854296 best_F: 0.8295559240832872 \n",
      "302000 :  0.11381553944710336\n",
      "304000 :  0.11972276425758546\n",
      "306000 :  0.121619420154455\n",
      "Train: new_F: 0.9275134138509664 best_F: 0.9262720664589823 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 39/49 [1:28:04<26:45, 160.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.8383299613757587 best_F: 0.8295559240832872 \n",
      "Saving Model to  ./models/self-trained-model\n",
      "308000 :  0.11664144409857119\n",
      "310000 :  0.11616878115870051\n",
      "312000 :  0.12096019394040328\n",
      "314000 :  0.11206340257079207\n",
      "Train: new_F: 0.9310594315245478 best_F: 0.9275134138509664 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%| | 40/49 [1:30:44<24:02, 160.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.8416239941477689 best_F: 0.8383299613757587 \n",
      "Saving Model to  ./models/self-trained-model\n",
      "316000 :  0.11151864337271898\n",
      "318000 :  0.10828746421226519\n",
      "320000 :  0.11369915930522129\n",
      "322000 :  0.11834130884093189\n",
      "Train: new_F: 0.9320673534400729 best_F: 0.9310594315245478 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%| | 41/49 [1:33:23<21:21, 160.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.8433339412730257 best_F: 0.8416239941477689 \n",
      "Saving Model to  ./models/self-trained-model\n",
      "324000 :  0.11700331319479018\n",
      "326000 :  0.10611281042894746\n",
      "328000 :  0.1064210467260498\n",
      "330000 :  0.106962676442389\n",
      "Train: new_F: 0.9312735276572109 best_F: 0.9320673534400729 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%| | 42/49 [1:36:03<18:40, 160.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.8425054308472121 best_F: 0.8433339412730257 \n",
      "332000 :  0.11708729477312126\n",
      "334000 :  0.11228107105122806\n",
      "336000 :  0.1085547551582366\n",
      "338000 :  0.10450001007386345\n",
      "Train: new_F: 0.9365894039735099 best_F: 0.9320673534400729 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 43/49 [1:38:43<15:59, 159.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.8473463432427504 best_F: 0.8433339412730257 \n",
      "Saving Model to  ./models/self-trained-model\n",
      "340000 :  0.11175668625900366\n",
      "342000 :  0.11550905822841881\n",
      "344000 :  0.10640509374631327\n",
      "346000 :  0.099734004329435\n",
      "Train: new_F: 0.9428149135041739 best_F: 0.9365894039735099 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 44/49 [1:41:22<13:18, 159.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.8496032478317032 best_F: 0.8473463432427504 \n",
      "Saving Model to  ./models/self-trained-model\n",
      "348000 :  0.1139439960292569\n",
      "350000 :  0.10656620034984973\n",
      "352000 :  0.09905831901174023\n",
      "354000 :  0.11084904537785756\n",
      "Train: new_F: 0.9428328529661808 best_F: 0.9428149135041739 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|| 45/49 [1:44:05<10:42, 160.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.8525911005310383 best_F: 0.8496032478317032 \n",
      "Saving Model to  ./models/self-trained-model\n",
      "356000 :  0.10995054078756047\n",
      "358000 :  0.10814929978790472\n",
      "360000 :  0.09956066692596098\n",
      "362000 :  0.0984378738039467\n",
      "Train: new_F: 0.9397918731417244 best_F: 0.9428328529661808 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|| 46/49 [1:46:45<08:01, 160.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.846237731733915 best_F: 0.8525911005310383 \n",
      "364000 :  0.1062900126310251\n",
      "366000 :  0.09868158547882976\n",
      "368000 :  0.1050247282804987\n",
      "370000 :  0.0995383498147542\n",
      "Train: new_F: 0.9419141231246767 best_F: 0.9428328529661808 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|| 47/49 [1:49:25<05:20, 160.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.8521549372613201 best_F: 0.8525911005310383 \n",
      "372000 :  0.10547487008368729\n",
      "374000 :  0.09935912152987152\n",
      "376000 :  0.10416621036425928\n",
      "378000 :  0.10662795761832208\n",
      "Train: new_F: 0.9460026157902385 best_F: 0.9428328529661808 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|| 48/49 [1:52:07<02:40, 161.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.856409315972859 best_F: 0.8525911005310383 \n",
      "Saving Model to  ./models/self-trained-model\n",
      "380000 :  0.09700220576986379\n",
      "382000 :  0.09974579065916159\n",
      "384000 :  0.09165878417930638\n",
      "386000 :  0.10041656721918132\n",
      "Train: new_F: 0.9486503869374884 best_F: 0.9460026157902385 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 49/49 [1:54:50<00:00, 140.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: new_F: 0.858814923189466 best_F: 0.856409315972859 \n",
      "Saving Model to  ./models/self-trained-model\n",
      "6890.203968048096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 512x384 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#parameters['reload']=False\n",
    "\n",
    "if not parameters['reload']:\n",
    "    tr = time.time()\n",
    "    model.train(True)\n",
    "    for epoch in tqdm(range(1,number_of_epochs)):\n",
    "        for i, index in enumerate(np.random.permutation(len(train_data))):\n",
    "            count += 1\n",
    "            data = train_data[index]\n",
    "\n",
    "            ##gradient updates for each data entry\n",
    "            model.zero_grad()\n",
    "\n",
    "            sentence_in = data['words']\n",
    "            sentence_in = Variable(torch.LongTensor(sentence_in))\n",
    "            tags = data['tags']\n",
    "            chars2 = data['chars']\n",
    "            \n",
    "            if parameters['char_mode'] == 'LSTM':\n",
    "                chars2_sorted = sorted(chars2, key=lambda p: len(p), reverse=True)\n",
    "                d = {}\n",
    "                for i, ci in enumerate(chars2):\n",
    "                    for j, cj in enumerate(chars2_sorted):\n",
    "                        if ci == cj and not j in d and not i in d.values():\n",
    "                            d[j] = i\n",
    "                            continue\n",
    "                chars2_length = [len(c) for c in chars2_sorted]\n",
    "                char_maxl = max(chars2_length)\n",
    "                chars2_mask = np.zeros((len(chars2_sorted), char_maxl), dtype='int')\n",
    "                for i, c in enumerate(chars2_sorted):\n",
    "                    chars2_mask[i, :chars2_length[i]] = c\n",
    "                chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "            \n",
    "            if parameters['char_mode'] == 'CNN':\n",
    "\n",
    "                d = {}\n",
    "\n",
    "                ## Padding the each word to max word size of that sentence\n",
    "                chars2_length = [len(c) for c in chars2]\n",
    "                char_maxl = max(chars2_length)\n",
    "                chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n",
    "                for i, c in enumerate(chars2):\n",
    "                    chars2_mask[i, :chars2_length[i]] = c\n",
    "                chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "\n",
    "\n",
    "            targets = torch.LongTensor(tags)\n",
    "\n",
    "            #we calculate the negative log-likelihood for the predicted tags using the predefined function\n",
    "            if use_gpu:\n",
    "                neg_log_likelihood = model.neg_log_likelihood(sentence_in.cuda(), targets.cuda(), chars2_mask.cuda(), chars2_length, d)\n",
    "            else:\n",
    "                neg_log_likelihood = model.neg_log_likelihood(sentence_in, targets, chars2_mask, chars2_length, d)\n",
    "            loss += neg_log_likelihood.item() / len(data['words'])\n",
    "            neg_log_likelihood.backward()\n",
    "\n",
    "            #we use gradient clipping to avoid exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), gradient_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            #Storing loss\n",
    "            if count % plot_every == 0:\n",
    "                loss /= plot_every\n",
    "                print(count, ': ', loss)\n",
    "                if losses == []:\n",
    "                    losses.append(loss)\n",
    "                losses.append(loss)\n",
    "                loss = 0.0\n",
    "\n",
    "            #Evaluating on Train, Test, Dev Sets\n",
    "            if count % (eval_every) == 0 and count > (eval_every * 20) or \\\n",
    "                    count % (eval_every*4) == 0 and count < (eval_every * 20):\n",
    "                model.train(False)\n",
    "                best_train_F, new_train_F, _ = evaluating(model, train_data, best_train_F,\"Train\")\n",
    "                best_dev_F, new_dev_F, save = evaluating(model, dev_data, best_dev_F,\"Dev\")\n",
    "                if save:\n",
    "                    print(\"Saving Model to \", model_name)\n",
    "                    torch.save(model.state_dict(), model_name)\n",
    "#                 best_test_F, new_test_F, _ = evaluating(model, test_data, best_test_F,\"Test\")\n",
    "\n",
    "                all_F.append([new_train_F, new_dev_F]) # new_test_F\n",
    "                model.train(True)\n",
    "\n",
    "            #Performing decay on the learning rate\n",
    "            if count % len(train_data) == 0:\n",
    "                adjust_learning_rate(optimizer, lr=learning_rate/(1+decay_rate*count/len(train_data)))\n",
    "\n",
    "    print(time.time() - tr)\n",
    "    plt.plot(losses)\n",
    "    plt.show()\n",
    "\n",
    "if not parameters['reload']:\n",
    "    #reload the best model saved from training\n",
    "    model.load_state_dict(torch.load(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-02T00:59:42.225055Z",
     "iopub.status.busy": "2024-04-02T00:59:42.224694Z",
     "iopub.status.idle": "2024-04-02T00:59:42.246211Z",
     "shell.execute_reply": "2024-04-02T00:59:42.245248Z",
     "shell.execute_reply.started": "2024-04-02T00:59:42.225017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "word : tag\n",
      "Jay : NA\n",
      "is : NA\n",
      "from : NA\n",
      "India : NA\n",
      "\n",
      "\n",
      "Donald : NA\n",
      "is : NA\n",
      "the : NA\n",
      "president : NA\n",
      "of : NA\n",
      "USA : NA\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_testing_sentences = ['Jay is from India','Donald is the president of USA']\n",
    "\n",
    "#parameters\n",
    "lower=parameters['lower']\n",
    "\n",
    "#preprocessing\n",
    "final_test_data = []\n",
    "for sentence in model_testing_sentences:\n",
    "    s=sentence.split()\n",
    "    str_words = [w for w in s]\n",
    "    words = [word_to_id[lower_case(w,lower) if lower_case(w,lower) in word_to_id else '<UNK>'] for w in str_words]\n",
    "    \n",
    "    # Skip characters that are not in the training set\n",
    "    chars = [[char_to_id[c] for c in w if c in char_to_id] for w in str_words]\n",
    "    \n",
    "    final_test_data.append({\n",
    "        'str_words': str_words,\n",
    "        'words': words,\n",
    "        'chars': chars,\n",
    "    })\n",
    "\n",
    "#prediction\n",
    "predictions = []\n",
    "print(\"Prediction:\")\n",
    "print(\"word : tag\")\n",
    "for data in final_test_data:\n",
    "    words = data['str_words']\n",
    "    chars2 = data['chars']\n",
    "\n",
    "    d = {} \n",
    "    \n",
    "    # Padding the each word to max word size of that sentence\n",
    "    chars2_length = [len(c) for c in chars2]\n",
    "    char_maxl = max(chars2_length)\n",
    "    chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n",
    "    for i, c in enumerate(chars2):\n",
    "        chars2_mask[i, :chars2_length[i]] = c\n",
    "    chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "\n",
    "    dwords = Variable(torch.LongTensor(data['words']))\n",
    "\n",
    "    # We are getting the predicted output from our model\n",
    "    if use_gpu:\n",
    "        val,predicted_id = model(dwords.cuda(), chars2_mask.cuda(), chars2_length, d)\n",
    "    else:\n",
    "        val,predicted_id = model(dwords, chars2_mask, chars2_length, d)\n",
    "\n",
    "    pred_chunks = get_chunks(predicted_id,tag_to_id)\n",
    "    temp_list_tags=['NA']*len(words)\n",
    "    for p in pred_chunks:\n",
    "        temp_list_tags[p[1]]=p[0]\n",
    "        \n",
    "    for word,tag in zip(words,temp_list_tags):\n",
    "        print(word,':',tag)\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
